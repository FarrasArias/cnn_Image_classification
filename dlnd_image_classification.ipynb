{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7cfc3393c8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(list(range(0,10)))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape=[None,*image_shape],name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,(None,n_classes),name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    t_shape = x_tensor.get_shape().as_list()\n",
    "    weight_shape = [conv_ksize[0],conv_ksize[1],t_shape[3],conv_num_outputs]\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.random_uniform(weight_shape,-1/np.sqrt(weight_shape[0]),1/np.sqrt(weight_shape[0])))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    #convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    #max_pooling\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,ksize=[1,pool_ksize[0],pool_ksize[1],1],strides=[1,pool_strides[0],pool_strides[1],1],padding=\"SAME\")\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    t_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.random_uniform([t_shape[1],num_outputs],-1/np.sqrt(t_shape[1]),1/np.sqrt(t_shape[1])))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    full_con = tf.add(tf.matmul(x_tensor,weights),bias)\n",
    "    full_con = tf.nn.relu(full_con)\n",
    "    return full_con\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    t_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.random_uniform([t_shape[1],num_outputs],-1/np.sqrt(t_shape[1]),1/np.sqrt(t_shape[1])))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    output = tf.add(tf.matmul(x_tensor,weights),bias)\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_mod = conv2d_maxpool(x,64,(3, 3),(2, 2),(2, 2),(2, 2))\n",
    "    conv_mod = conv2d_maxpool(conv_mod,128,(3, 3),(1, 1),(2, 2),(1, 1))\n",
    "    conv_mod = conv2d_maxpool(conv_mod,256,(3, 3),(2, 2),(2, 2),(2, 2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv_mod = flatten(conv_mod)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    conv_mod = fully_conn(conv_mod,8192)\n",
    "    conv_mod = tf.nn.dropout(conv_mod,keep_prob)\n",
    "    conv_mod = fully_conn(conv_mod,4096)\n",
    "    conv_mod = fully_conn(conv_mod,2048)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    conv_mod = output(conv_mod,10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return conv_mod\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    opt = session.run(optimizer, feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost,feed_dict={x: feature_batch,y: label_batch,keep_prob: 1.0})\n",
    "    acc = session.run(accuracy,feed_dict={x: valid_features,y: valid_labels,keep_prob: 1.0})\n",
    "    print (\"loss\",loss,\"acc\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.07333 acc 0.2696\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 1.84002 acc 0.3288\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 1.5913 acc 0.3974\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 1.384 acc 0.4126\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.27681 acc 0.422\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.14367 acc 0.446\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 0.997015 acc 0.4728\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 0.911752 acc 0.4814\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 0.800946 acc 0.4932\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 0.695578 acc 0.5054\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 0.5489 acc 0.515\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 0.596181 acc 0.4646\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 0.4454 acc 0.5114\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 0.365861 acc 0.5076\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 0.303012 acc 0.519\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 0.318202 acc 0.5134\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 0.241614 acc 0.509\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 0.267161 acc 0.462\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 0.197111 acc 0.4606\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 0.135801 acc 0.494\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 0.112291 acc 0.495\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.0965178 acc 0.511\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.0612967 acc 0.4906\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.0682588 acc 0.518\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.0692199 acc 0.5296\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.0342923 acc 0.5312\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.0283384 acc 0.5164\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.0360764 acc 0.5156\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.0362067 acc 0.506\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.0160732 acc 0.5082\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.0246805 acc 0.5048\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.026008 acc 0.52\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.0106781 acc 0.5136\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.00686591 acc 0.5114\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.00439286 acc 0.525\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.00145044 acc 0.5268\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.00573851 acc 0.5186\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.000531473 acc 0.5264\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.00347502 acc 0.5286\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.00640579 acc 0.5202\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.00282719 acc 0.5182\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.00451033 acc 0.5146\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.0189915 acc 0.5272\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.00296061 acc 0.5016\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.00315577 acc 0.519\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.00166538 acc 0.5288\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.00173482 acc 0.5264\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.000860312 acc 0.519\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.00431093 acc 0.5272\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.00228143 acc 0.535\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.000888193 acc 0.5194\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.00897837 acc 0.5092\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.00158429 acc 0.513\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.00164211 acc 0.5262\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.000628481 acc 0.52\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.0045391 acc 0.5296\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.0013001 acc 0.5\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.00103036 acc 0.5202\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.00590294 acc 0.5164\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.000387371 acc 0.5144\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.00139796 acc 0.5086\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.00159205 acc 0.5316\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.00378116 acc 0.535\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.000344701 acc 0.5314\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.000300634 acc 0.5188\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.000826342 acc 0.5336\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.00217615 acc 0.5312\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.000236299 acc 0.514\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.0015304 acc 0.5264\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.00114225 acc 0.528\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.0057389 acc 0.533\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.00154679 acc 0.5274\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.00106942 acc 0.5174\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.00209151 acc 0.528\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.000235877 acc 0.533\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.000191779 acc 0.5274\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.000273552 acc 0.5378\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.000486984 acc 0.5244\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.000229054 acc 0.5318\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.000377223 acc 0.5344\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.00240146 acc 0.5268\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.00231987 acc 0.5272\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.000290699 acc 0.5314\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.000183394 acc 0.5252\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.00136733 acc 0.53\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.000242045 acc 0.5308\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.00121484 acc 0.5196\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.00148581 acc 0.5362\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.000515131 acc 0.5214\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.0026209 acc 0.5136\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.00241691 acc 0.5364\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.00242532 acc 0.5252\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.000217082 acc 0.5352\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.000410267 acc 0.5328\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.000277992 acc 0.5274\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.0011509 acc 0.5354\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.000283635 acc 0.5296\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.00331699 acc 0.5306\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 6.19731e-05 acc 0.5324\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.000781916 acc 0.5298\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 2.121 acc 0.2438\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss 1.84696 acc 0.2398\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss 1.44435 acc 0.383\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss 1.47504 acc 0.4092\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss 1.50378 acc 0.4214\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 1.74112 acc 0.4692\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss 1.45999 acc 0.4616\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss 1.13855 acc 0.4874\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss 1.09312 acc 0.4996\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss 1.42909 acc 0.4818\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 1.41297 acc 0.5184\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss 1.12613 acc 0.5176\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss 0.910886 acc 0.5398\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss 0.986884 acc 0.5174\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss 1.12604 acc 0.5546\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 1.22159 acc 0.5508\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss 0.915927 acc 0.548\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss 0.770056 acc 0.5698\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss 0.798498 acc 0.5694\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss 1.00595 acc 0.5744\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.05643 acc 0.5646\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss 0.733825 acc 0.5756\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss 0.826319 acc 0.5776\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss 0.678926 acc 0.583\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss 0.813186 acc 0.6022\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 0.883026 acc 0.593\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss 0.670554 acc 0.576\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss 0.695318 acc 0.588\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss 0.612383 acc 0.593\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss 0.720737 acc 0.6088\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 0.839533 acc 0.602\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss 0.601174 acc 0.5728\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss 0.615983 acc 0.593\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss 0.466412 acc 0.617\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss 0.55801 acc 0.5998\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 0.694721 acc 0.6126\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss 0.528302 acc 0.5912\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss 0.551133 acc 0.5898\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss 0.407452 acc 0.6334\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss 0.43829 acc 0.6126\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 0.644988 acc 0.6146\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss 0.503162 acc 0.5558\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss 0.449673 acc 0.5666\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss 0.334711 acc 0.6306\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss 0.347131 acc 0.6168\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 0.598914 acc 0.6154\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss 0.40515 acc 0.5942\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss 0.27552 acc 0.6124\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss 0.282868 acc 0.6396\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss 0.298253 acc 0.6394\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 0.395933 acc 0.6226\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss 0.235194 acc 0.6144\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss 0.270743 acc 0.6184\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss 0.25079 acc 0.6338\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss 0.300668 acc 0.6194\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 0.349508 acc 0.6296\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss 0.287459 acc 0.5958\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss 0.182683 acc 0.6296\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss 0.217321 acc 0.6394\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss 0.272338 acc 0.6018\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 0.313803 acc 0.6334\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss 0.176564 acc 0.615\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss 0.15775 acc 0.6292\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss 0.192748 acc 0.6382\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss 0.256667 acc 0.607\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 0.318698 acc 0.6362\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss 0.180514 acc 0.6174\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss 0.208586 acc 0.6006\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss 0.101958 acc 0.634\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss 0.214318 acc 0.6176\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 0.203259 acc 0.6252\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss 0.146033 acc 0.618\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss 0.0979645 acc 0.636\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss 0.135154 acc 0.625\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss 0.133552 acc 0.6388\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 0.172269 acc 0.6374\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss 0.12341 acc 0.62\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss 0.0702645 acc 0.6362\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss 0.107316 acc 0.6132\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss 0.103711 acc 0.6344\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 0.141977 acc 0.6374\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss 0.0920585 acc 0.6326\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss 0.0384195 acc 0.6506\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss 0.0916667 acc 0.638\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss 0.0740973 acc 0.6294\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 0.149067 acc 0.6342\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss 0.0572819 acc 0.6308\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss 0.0566073 acc 0.6448\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss 0.0646857 acc 0.6366\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss 0.0704437 acc 0.639\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 0.130038 acc 0.6426\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss 0.0472246 acc 0.633\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss 0.036399 acc 0.6466\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss 0.0659432 acc 0.635\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss 0.0413516 acc 0.6386\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 0.0801413 acc 0.6418\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss 0.0600393 acc 0.6384\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss 0.0459592 acc 0.6354\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss 0.0744044 acc 0.6402\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss 0.0738893 acc 0.6238\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 0.0773292 acc 0.6332\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss 0.0300102 acc 0.641\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss 0.0345692 acc 0.6374\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss 0.0687033 acc 0.6442\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss 0.0336193 acc 0.6434\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.072573 acc 0.6398\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss 0.0528518 acc 0.6396\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss 0.0372993 acc 0.6478\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss 0.0613486 acc 0.6478\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss 0.0399687 acc 0.6396\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.0727697 acc 0.6416\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss 0.0456338 acc 0.6356\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss 0.0293843 acc 0.6216\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss 0.0250188 acc 0.6476\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss 0.0501394 acc 0.6508\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.0622868 acc 0.633\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss 0.0357412 acc 0.6514\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss 0.0227327 acc 0.6232\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss 0.0165202 acc 0.6552\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss 0.0193656 acc 0.647\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.0334919 acc 0.645\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss 0.0146646 acc 0.6378\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss 0.0103233 acc 0.64\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss 0.0141759 acc 0.6412\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss 0.0139977 acc 0.6458\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.0268864 acc 0.6514\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss 0.0147613 acc 0.6406\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss 0.0125131 acc 0.632\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss 0.0119152 acc 0.6428\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss 0.0252003 acc 0.645\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.0331098 acc 0.6488\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss 0.0261032 acc 0.6572\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss 0.0211169 acc 0.647\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss 0.0268684 acc 0.639\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss 0.00960629 acc 0.6428\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.0242738 acc 0.6526\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss 0.0377412 acc 0.6382\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss 0.0205827 acc 0.6298\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss 0.0212689 acc 0.6486\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss 0.0132268 acc 0.6462\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.0814781 acc 0.6466\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss 0.0166359 acc 0.6482\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss 0.0158984 acc 0.6352\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss 0.0287929 acc 0.6612\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss 0.02019 acc 0.6622\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.0216174 acc 0.6448\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss 0.02341 acc 0.6432\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss 0.016731 acc 0.6536\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss 0.04756 acc 0.6474\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss 0.0104791 acc 0.6636\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.0276631 acc 0.6418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, CIFAR-10 Batch 2:  loss 0.0759704 acc 0.651\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss 0.00792591 acc 0.6468\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss 0.00914369 acc 0.6582\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss 0.00543325 acc 0.6544\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.0176019 acc 0.6432\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss 0.0156627 acc 0.6548\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss 0.00536772 acc 0.6538\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss 0.00721502 acc 0.649\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss 0.0323725 acc 0.6438\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.0114867 acc 0.6364\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss 0.00368366 acc 0.6522\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss 0.0119137 acc 0.662\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss 0.0106435 acc 0.6482\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss 0.00872333 acc 0.6566\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.0137015 acc 0.6372\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss 0.0383291 acc 0.6404\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss 0.0026407 acc 0.6508\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss 0.0132822 acc 0.6496\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss 0.0246794 acc 0.651\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.0212949 acc 0.6382\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss 0.0111428 acc 0.6326\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss 0.0041796 acc 0.6596\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss 0.0135421 acc 0.6556\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss 0.0115945 acc 0.661\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.0168349 acc 0.637\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss 0.0127256 acc 0.6522\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss 0.00635557 acc 0.6588\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss 0.0175143 acc 0.6582\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss 0.00819927 acc 0.6664\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.0260851 acc 0.6482\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss 0.00437663 acc 0.645\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss 0.00162663 acc 0.6568\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss 0.004759 acc 0.658\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss 0.00993048 acc 0.658\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.0119523 acc 0.6544\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss 0.00281342 acc 0.6606\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss 0.00379444 acc 0.667\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss 0.00800833 acc 0.6584\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss 0.0154321 acc 0.6514\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.0240368 acc 0.6318\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss 0.0135582 acc 0.647\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss 0.00511802 acc 0.6624\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss 0.00732262 acc 0.6632\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss 0.00599167 acc 0.6582\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.0349418 acc 0.6422\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss 0.0046619 acc 0.6482\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss 0.00690678 acc 0.6638\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss 0.0171356 acc 0.6444\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss 0.00505482 acc 0.6698\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.00437521 acc 0.6648\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss 0.0150573 acc 0.6456\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss 0.00348229 acc 0.6606\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss 0.0150093 acc 0.653\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss 0.0134988 acc 0.662\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.00760254 acc 0.6368\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss 0.00263392 acc 0.6632\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss 0.000803742 acc 0.6606\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss 0.00256791 acc 0.6588\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss 0.00664428 acc 0.664\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.0114267 acc 0.6472\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss 0.00388481 acc 0.6558\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss 0.00377582 acc 0.6488\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss 0.0016057 acc 0.6606\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss 0.00289837 acc 0.6582\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.00574331 acc 0.6622\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss 0.00538045 acc 0.6586\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss 0.00126271 acc 0.6666\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss 0.00809057 acc 0.6644\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss 0.0445314 acc 0.6658\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.0118034 acc 0.657\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss 0.00558998 acc 0.652\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss 0.00169229 acc 0.6544\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss 0.0113521 acc 0.6546\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss 0.000998214 acc 0.6704\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.0253052 acc 0.6504\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss 0.0105189 acc 0.6598\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss 0.00354259 acc 0.6624\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss 0.0120805 acc 0.6618\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss 0.00405569 acc 0.6594\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.0163674 acc 0.6656\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss 0.00119118 acc 0.65\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss 0.00323791 acc 0.6568\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss 0.00575201 acc 0.6666\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss 0.0101461 acc 0.6642\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.00673834 acc 0.6586\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss 0.00401384 acc 0.6386\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss 0.00324927 acc 0.6632\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss 0.0100054 acc 0.6536\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss 0.00681228 acc 0.6594\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.0518334 acc 0.6544\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss 0.000357027 acc 0.656\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss 0.00400151 acc 0.6732\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss 0.00117589 acc 0.6618\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss 0.0099573 acc 0.6614\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.0155476 acc 0.6528\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss 0.00183019 acc 0.659\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss 0.000864196 acc 0.6624\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss 0.00151329 acc 0.6584\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss 0.00275798 acc 0.671\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.00314603 acc 0.6648\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss 0.0033541 acc 0.6572\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss 0.00252975 acc 0.6588\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss 0.000782543 acc 0.6654\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss 0.00189466 acc 0.661\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.00466949 acc 0.6484\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss 0.00272136 acc 0.665\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss 0.000836716 acc 0.6688\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss 0.00787325 acc 0.6558\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss 0.00627239 acc 0.662\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.0174474 acc 0.6668\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss 0.00570778 acc 0.6544\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss 0.00192063 acc 0.6604\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss 0.0113208 acc 0.666\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss 0.00324586 acc 0.6506\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.00245949 acc 0.657\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss 0.00372124 acc 0.659\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss 0.00184701 acc 0.663\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss 0.021523 acc 0.6718\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss 0.00377478 acc 0.6766\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.00268426 acc 0.6594\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss 0.00367506 acc 0.6682\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss 0.0159756 acc 0.6622\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss 0.00292735 acc 0.6664\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss 0.0118657 acc 0.6684\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.00603536 acc 0.6682\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss 0.00143213 acc 0.6712\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss 0.005626 acc 0.662\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss 0.00386184 acc 0.6612\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss 0.00397054 acc 0.6702\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.00389316 acc 0.6678\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss 0.00221907 acc 0.6602\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss 0.00173805 acc 0.6748\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss 0.000579546 acc 0.672\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss 0.00861692 acc 0.6646\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.00502666 acc 0.6704\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss 0.0192347 acc 0.6816\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss 0.00204134 acc 0.6778\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss 0.00131578 acc 0.6678\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss 0.0077527 acc 0.6646\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.00966385 acc 0.6626\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss 0.0022613 acc 0.6738\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss 0.00127535 acc 0.6696\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss 0.000931448 acc 0.6646\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss 0.0024952 acc 0.668\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.00409607 acc 0.6736\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss 0.00401545 acc 0.6716\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss 0.0032013 acc 0.6746\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss 0.000538724 acc 0.6758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, CIFAR-10 Batch 5:  loss 0.00430117 acc 0.6638\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.0122395 acc 0.666\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss 0.000611573 acc 0.6672\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss 0.00125927 acc 0.6742\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss 0.00664915 acc 0.6706\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss 0.00144526 acc 0.6758\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.0021688 acc 0.6644\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss 0.00338142 acc 0.672\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss 0.00149277 acc 0.6748\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss 0.000688869 acc 0.6638\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss 0.0151542 acc 0.6728\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.00338336 acc 0.6494\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss 0.000287088 acc 0.6662\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss 0.00252438 acc 0.6756\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss 0.00078451 acc 0.6706\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss 0.00243955 acc 0.672\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.000302907 acc 0.671\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss 0.000651155 acc 0.6642\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss 0.00807894 acc 0.6728\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss 0.00798208 acc 0.663\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss 0.00314432 acc 0.6678\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.00553498 acc 0.664\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss 0.00313304 acc 0.6744\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss 0.00113597 acc 0.6762\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss 0.00138776 acc 0.6896\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss 0.00400091 acc 0.6796\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.0019559 acc 0.673\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss 0.00118805 acc 0.6694\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss 0.00010128 acc 0.6816\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss 0.000973483 acc 0.6662\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss 0.00969177 acc 0.6764\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.000987054 acc 0.6734\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss 0.00466086 acc 0.6764\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss 0.00373952 acc 0.668\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss 0.00190054 acc 0.6746\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss 0.00499565 acc 0.6686\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.00692778 acc 0.6592\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss 0.000793917 acc 0.6778\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss 0.00698376 acc 0.6662\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss 0.00289604 acc 0.674\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss 0.00123583 acc 0.6712\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.000825985 acc 0.6694\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss 0.000606253 acc 0.6744\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss 0.00149715 acc 0.6824\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss 0.00226549 acc 0.6708\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss 0.00444627 acc 0.6674\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.0281779 acc 0.6682\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss 0.000816088 acc 0.6698\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss 0.00238275 acc 0.6736\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss 0.0063914 acc 0.6708\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss 0.00677252 acc 0.6806\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.00289125 acc 0.6716\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss 0.0011266 acc 0.6634\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss 0.00141075 acc 0.6662\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss 0.00124631 acc 0.6826\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss 0.00470076 acc 0.6824\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.000880168 acc 0.6828\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss 0.00136596 acc 0.6744\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss 0.00527606 acc 0.6778\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss 0.00524669 acc 0.6664\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss 0.000863734 acc 0.6738\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.000233875 acc 0.6888\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss 0.00365175 acc 0.6648\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss 0.00226901 acc 0.6782\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss 0.00220993 acc 0.6568\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss 0.00305322 acc 0.6716\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.000982281 acc 0.6776\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss 0.00077309 acc 0.668\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss 0.000382517 acc 0.6748\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss 0.000274227 acc 0.6882\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss 0.00454618 acc 0.6694\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.00896473 acc 0.671\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss 0.00549959 acc 0.6704\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss 0.0043684 acc 0.6712\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss 0.000732496 acc 0.6636\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss 0.00137771 acc 0.6652\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.00682651 acc 0.6724\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss 0.00501146 acc 0.6776\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss 0.00178489 acc 0.6724\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss 0.000536132 acc 0.675\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss 0.00268304 acc 0.6776\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.00121505 acc 0.6746\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss 0.00165529 acc 0.682\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss 0.00105793 acc 0.6786\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss 0.00258037 acc 0.6596\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss 0.00431252 acc 0.682\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.00608884 acc 0.671\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss 0.00286937 acc 0.6684\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss 0.000157352 acc 0.689\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss 0.00741489 acc 0.672\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss 0.00807069 acc 0.6774\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.000958566 acc 0.6714\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss 0.00354001 acc 0.6738\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss 0.00226579 acc 0.6682\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss 0.00272232 acc 0.6714\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss 0.0013543 acc 0.679\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.00117814 acc 0.6702\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss 0.0031959 acc 0.6674\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss 0.00058757 acc 0.6796\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss 0.000247413 acc 0.6774\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss 0.00157154 acc 0.6778\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.00371044 acc 0.6656\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss 0.000381881 acc 0.6754\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss 0.00155356 acc 0.6836\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss 0.00090204 acc 0.6788\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss 0.000916052 acc 0.6772\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.000971676 acc 0.6742\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss 0.000409606 acc 0.6686\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss 0.00304342 acc 0.6564\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss 0.00202839 acc 0.6704\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss 0.00327859 acc 0.6796\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.00147795 acc 0.6838\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss 0.0950391 acc 0.6738\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss 0.000603718 acc 0.6794\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss 0.000286011 acc 0.6742\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss 0.00213084 acc 0.6698\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.00140501 acc 0.6706\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss 0.00117363 acc 0.6658\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss 0.00294745 acc 0.6786\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss 0.000832428 acc 0.669\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss 0.0028245 acc 0.6704\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.0758892 acc 0.6734\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss 0.0010593 acc 0.673\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss 0.00129334 acc 0.6672\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss 0.00084603 acc 0.6828\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss 0.000360928 acc 0.6756\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.00309308 acc 0.6602\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss 0.00457618 acc 0.6644\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss 0.00304198 acc 0.66\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss 0.00173131 acc 0.6786\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss 0.00184726 acc 0.685\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.0112891 acc 0.6514\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss 0.000540356 acc 0.673\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss 0.000120567 acc 0.6784\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss 0.00264864 acc 0.6848\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss 0.000948053 acc 0.681\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.00274193 acc 0.6726\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss 0.000647253 acc 0.677\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss 0.000268943 acc 0.676\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss 0.0743815 acc 0.6712\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss 0.00294479 acc 0.6856\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.00165004 acc 0.6724\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss 0.0034008 acc 0.6496\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss 0.00107744 acc 0.68\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss 0.00227107 acc 0.6818\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss 0.00179628 acc 0.678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.00412201 acc 0.6766\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss 0.00035503 acc 0.677\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss 0.00462132 acc 0.6728\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss 0.00369816 acc 0.6798\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss 0.00128754 acc 0.68\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.000768989 acc 0.673\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss 0.00157252 acc 0.6796\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss 0.00169774 acc 0.676\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss 0.0147841 acc 0.6784\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss 0.00122289 acc 0.6804\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.000428717 acc 0.6738\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss 0.000245173 acc 0.6778\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss 0.00761695 acc 0.652\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss 0.000412064 acc 0.6764\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss 0.00113044 acc 0.6716\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.00231677 acc 0.6808\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss 0.000220637 acc 0.6766\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss 0.0100857 acc 0.6592\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss 0.00323127 acc 0.6752\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss 0.0200783 acc 0.6728\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.000787463 acc 0.6782\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss 0.0018219 acc 0.6794\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss 0.00109574 acc 0.6804\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss 0.00234223 acc 0.679\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss 0.00297854 acc 0.6722\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.000449686 acc 0.6744\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss 0.00334805 acc 0.675\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss 0.0106477 acc 0.6528\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss 0.0030899 acc 0.678\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss 0.00310827 acc 0.6804\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.00597522 acc 0.6682\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss 0.000135037 acc 0.6808\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss 0.000237764 acc 0.6816\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss 0.00221617 acc 0.669\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss 0.00232594 acc 0.6692\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.00134847 acc 0.6708\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss 0.001692 acc 0.6756\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss 0.000957157 acc 0.6912\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss 0.00172616 acc 0.6796\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss 0.00488656 acc 0.6672\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.000490742 acc 0.688\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss 0.00333717 acc 0.6768\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss 0.000187336 acc 0.6786\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss 0.00122546 acc 0.685\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss 0.00144559 acc 0.663\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.013145 acc 0.675\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss 0.000360569 acc 0.6784\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss 0.000840014 acc 0.6746\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss 0.003209 acc 0.6908\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss 0.00127622 acc 0.6818\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.00221192 acc 0.684\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss 0.000657921 acc 0.6856\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss 0.000683873 acc 0.6906\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss 0.00038632 acc 0.679\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss 0.00464301 acc 0.6624\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6624802215189873\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMzMzCz7//0nevcswyRXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ce868ffd0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
